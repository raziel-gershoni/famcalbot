[{"TimeUTC":"2025-11-23 07:38:45","timestampInMs":1763883525657,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"82nkc-1763883514818-bcdb242651d0","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 319,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTCYZT8DQFPT1WZ1TND8CB","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:38:39","timestampInMs":1763883519330,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"82nkc-1763883514818-bcdb242651d0","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Model testing completed, lock released","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTCYZT8DQFPT1WZ1TND8CB","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:38:39","timestampInMs":1763883519031,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"82nkc-1763883514818-bcdb242651d0","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTCYZT8DQFPT1WZ1TND8CB","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:38:39","timestampInMs":1763883519031,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"82nkc-1763883514818-bcdb242651d0","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:38:39 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '12',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '26',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '375ms',\n    'x-request-id': 'req_b7efa2ff2b98469ba98aa7577210ae54',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=FqwsBRru4nliHhUD2mKrrn7hkg1UtcLcD6L4eAG.zXM-1763883519-1.0.1.1-vhV7_x9jqiSGPcWRkbeIgZ6W0HBSMFWW0IKfQgXuFIn8av3eiVYnXDH.jNABCeZM01zdYTgc5BSWqts360TaCACU3069vQhg0F43RNaUXFk; path=/; expires=Sun, 23-Nov-25 08:08:39 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=8yuQi.CgTa7uyYfDluNfoU5fg4PdWPwZ.zbD6WiSX7g-1763883519028-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f02196c988012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_b7efa2ff2b98469ba98aa7577210ae54',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTCYZT8DQFPT1WZ1TND8CB","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:38:34","timestampInMs":1763883514946,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"82nkc-1763883514818-bcdb242651d0","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:38:34 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '25',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198747',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '375ms',\n    'x-request-id': 'req_09b5023254ea4ba79cba3b52765595f9',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=k.L4dkcjyVnwcDg.b.Q467WO2LH6HjSBjOkaotG.KZ8-1763883514-1.0.1.1-BY3_IQM0QiP5rDIVSD_gvNGqtXkxtH9ZF8HsUCH8MlTv08.8fuKuOM4Cl88uQTDX0UprluqVqieQ04O5QnLulrSAZPJV1TwnIYWnaSV7CPU; path=/; expires=Sun, 23-Nov-25 08:08:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=L9liGeN8PqSXaEyu1ljhdQ4.zuKimgtGuGXagEa4mYk-1763883514942-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f020019908012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_09b5023254ea4ba79cba3b52765595f9',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTCYZT8DQFPT1WZ1TND8CB","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:38:34","timestampInMs":1763883514874,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"82nkc-1763883514818-bcdb242651d0","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":10876,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTCYZT8DQFPT1WZ1TND8CB","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:37:43","timestampInMs":1763883463996,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:37:43 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '22',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '375ms',\n    'x-request-id': 'req_457593ed743e4402a07a7556928ab40f',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=y1zUGMjqAlLxrZKS6rwQUgxhQootITrQifbXZ6HlM8U-1763883463-1.0.1.1-lfqyHANslLKYNkNov1kCgtAIF1GGhorBayiqhEwYr9je7iHRXjiglgcK7qk17kavowDDS7sKbdvymEHV5e2EmaCLgUiAKr8pDDZdfiizh.Y; path=/; expires=Sun, 23-Nov-25 08:07:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=i6AGf9rORA2mei.tpwOCN2UiswnxjjWaXImTR6eQDmg-1763883463992-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f00c17b3b8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_457593ed743e4402a07a7556928ab40f',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:43","timestampInMs":1763883463954,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 322,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:42","timestampInMs":1763883462910,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:37:42 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '17',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '41',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '375ms',\n    'x-request-id': 'req_758b1b4c8b524c74ac2da79b1512ee8e',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=MMoXvu1Gwvo4WsUM_MvHkEd_Jbj4TVvmmKbAIF.CXTw-1763883462-1.0.1.1-pmqzzKZXN6I5yN9J11UGlkkPhOmP14DsaS.y.b2Ashe3Z6rqjdq6VrZQwLMXTUMVHy4JA1GfLIqUuxT._hvTghYNbuLiQEWUPJysl6QqIFs; path=/; expires=Sun, 23-Nov-25 08:07:42 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=ML2LvlMmFvZMp5zhlYcyp.ka8p9eLDcPvN.IkmLl_9Q-1763883462906-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f00babfad8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_758b1b4c8b524c74ac2da79b1512ee8e',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:42","timestampInMs":1763883462704,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:42","timestampInMs":1763883462704,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:37:42 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '23',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '336ms',\n    'x-request-id': 'req_8f70d36ef88248c1b2dfe612cc35e5b7',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=nJ330.rM.oA5NMXwMWAUpbqGPdnxm.O.9FILGNxaTPw-1763883462-1.0.1.1-xrt0slLVLcpV8yvvushLOaMDbdER0E0mep1V5l08UdpA.01R3nTydwJArucXhQrFiOCnbU088nZDzv10Y4rq3eK4ruv0ZdaSjstWvrzwVV8; path=/; expires=Sun, 23-Nov-25 08:07:42 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=brCv1cMyvoV26SGLyi7HL8PU_yTAeaephf.gs50E.i0-1763883462700-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f00b97f1a8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_8f70d36ef88248c1b2dfe612cc35e5b7',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:38","timestampInMs":1763883458629,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:37:38 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '18',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '34',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '336ms',\n    'x-request-id': 'req_56422300559a448c8500174f985e3086',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=Uma9AvHv7As4Oe7MRdi4L5YKUoNH32q5o0eZCZKB4Ec-1763883458-1.0.1.1-HUOuiCpFFgQMiKh5rg9Bjszyq.8xaUVeus3eH2rjJgw2ES3AcJ47sVKkAuINK0jHr4vpxzd43SoL3HuP_ln2BN9QDgZxJYsSYZxX.4lqpsQ; path=/; expires=Sun, 23-Nov-25 08:07:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=BM9ImY85MLdIK85TFhwLp9uXOHq3OaG36rf86y.120A-1763883458625-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f009fd85d8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_56422300559a448c8500174f985e3086',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:36","timestampInMs":1763883456534,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:37:36 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '12',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '31',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '336ms',\n    'x-request-id': 'req_ce7510f9f5b54f8abc37c92a0dd1627a',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=8.fK2ci0afDTRt46qQzKMUtb7aZIukSE4dggQYGFFXQ-1763883456-1.0.1.1-6Jy7g.lGh4TlwOfGXRaTuOSlmIKJgug4gxE4QS.tbKK9CrhOK60Ivnvn1SKOL8qY701j9hmiPMCHyziKVZZvUSZBcUEoMdyeGPHpsRHRL2c; path=/; expires=Sun, 23-Nov-25 08:07:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=ehQhZyxTy4sdth6fz0JGtgFQdwaQNzEzl4QhLiej3OY-1763883456530-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f0092f9608012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_ce7510f9f5b54f8abc37c92a0dd1627a',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:35","timestampInMs":1763883455470,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:37:35 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '19',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '54',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '200000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '198880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '336ms',\n    'x-request-id': 'req_5c54d46fcf814a6ca71e371a9f37e021',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=hdz7YdRHAM4Kb7GjveB6CPgxJxKqoU6KTcw7rnf0qKE-1763883455-1.0.1.1-fLYUx440hBfZFtnusqmSosMvWIgd.NXv8LFfI0pk62WR1NUX5ZQWwuYJAONGZWd_zbeX0sRlqu0LlnqL0.WvcH0xQnl39h5gP4FLHK8wB7s; path=/; expires=Sun, 23-Nov-25 08:07:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=jntEnoqsNcJTdmBp8MgHC4D6LKkhKlv6Q4y0AFm2q0g-1763883455466-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2f008c2de38012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_5c54d46fcf814a6ca71e371a9f37e021',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:35","timestampInMs":1763883455376,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 7/7: gpt-5-nano","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:33","timestampInMs":1763883453267,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:33","timestampInMs":1763883453267,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:50 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '23',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_024989497caf4e1ca64cb5b8aca17f8f',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=jPHL2p8IucYO4LzropuZvAOqr4m3T1BMDb7JE7Vovf4-1763883410-1.0.1.1-8tQj_FnaW1EB5gBXFIccoDHRRK25L99zc2UucXsWMKLs9knyTgFidiXlr9lpGdzDAQGDdmL9HEugTg_l9x5wPHnbfuB0JdF0ATa8s6sHzgA; path=/; expires=Sun, 23-Nov-25 08:06:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=aRHSmW0XoxLacBU0r5fQE.vUch9Q9aiLeGtV25DKQ_g-1763883410374-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff725c028012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_024989497caf4e1ca64cb5b8aca17f8f',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:37:33","timestampInMs":1763883453241,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"rh6f8-1763883453170-ecf7a02eb208","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":10857,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQTB2SS08B4Y3S527VXS12Y","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:36:50","timestampInMs":1763883410077,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 319,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:46","timestampInMs":1763883406295,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:46 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '11',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '25',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_236111ee442842f79b41278b4905876a',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=cp2eELefmZqaGpx21Tshq13nUKKX.JBCZ63_e8ZLIKw-1763883406-1.0.1.1-CUXRv2ZIANNOdrslb7ErEB3NBP5OfoQN4AxvFThFill.Toy7IPOjL5FXPMQTzzPJRhDF_VRWCQmDVgzBGlZOvExCPqFcevIHarFWLLJegVM; path=/; expires=Sun, 23-Nov-25 08:06:46 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=OCQJ_iECpzmtQIEtZMGGiT9sCa3Z7hRcyiEAGf6pOXc-1763883406291-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff58ff308012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_236111ee442842f79b41278b4905876a',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:44","timestampInMs":1763883404234,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:44 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '24',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_89d2a4582dd641329f741dede1212e56',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=ZZE_R9UhO7tShR_pD7SWUXFEhcgHvukCohMrQL9tug8-1763883404-1.0.1.1-x6JX9z5jAtAoMqWTs5EP6PdKnGtOycXA0veQGxVHU9r.05y.DMyBNSp4d8yn8ifsQEsKS7yDeqBa3yDkroCG1MMeGwIdZHX2ZWDOM4GQ_Vc; path=/; expires=Sun, 23-Nov-25 08:06:44 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=PE5hGCerYvDsJSxXf027amqj5NLeq4bPfBb.oZfFKLo-1763883404230-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff4c180a8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_89d2a4582dd641329f741dede1212e56',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:43","timestampInMs":1763883403176,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:43 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '12',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '29',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_27eec05292cf47b89238de0c37a24a89',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=8jm6Tp0jtA56tN.fti7bj6ACkau9v.TGLEuY5wLhQx4-1763883403-1.0.1.1-bjtBMOoPCV8u8.928PdHRML9Sh8_21gAGL9DCk4Yrp3eKsqDKD9dTleiZQW..P2xFwxOJVjDT9drzkYjXBboR0dbX.P2JIiIpt6PHUQcwgA; path=/; expires=Sun, 23-Nov-25 08:06:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=pdd_Pxo06ENwHyJ.yJW8dBBccaUavAIwdRPfkauoAII-1763883403172-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff457cc68012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_27eec05292cf47b89238de0c37a24a89',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:42","timestampInMs":1763883402982,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:42","timestampInMs":1763883402982,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:42 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '21',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_a6262a7af5334129abc881e10bc85b8a',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=HgkORxNVni.clFk3xtCqFsH1boorNmoVExL26BhHweM-1763883402-1.0.1.1-c0cV12SndZI8QdmqNma_grunlFSf36U1wHU3qrb6mVeayk1FNss9Iz1lEvCT.AtzDmxqp4tYp4nU5cirZOWywRxYlneRG4GgmMdxfxBGZYg; path=/; expires=Sun, 23-Nov-25 08:06:42 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=cxhzx7tsaeeDLcxyD7OQrsm2y22guxWnGlv87KusVS4-1763883402977-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff442c1f8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_a6262a7af5334129abc881e10bc85b8a',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:38","timestampInMs":1763883398905,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:38 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '31',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '46',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_428c586e4cd14549b78aca3661693664',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=qKrYvURoOe3bsmZKnRVdFmXWfNoiuZ6oogyJPqw6B1k-1763883398-1.0.1.1-QKG2Tj6jLKy60WXaJowDrQaBeZ3C5Pi2Afg1g38i4tTiGxWJYIh0g25L2xGP5znFkl6_h.AHntZpzWUS.uJpzcDdPR_IHrfJTzyrplMvBMY; path=/; expires=Sun, 23-Nov-25 08:06:38 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=iXlA1JGj1ztZR_fBZBAQ.iar2fLcVp38ebX2nCp4Gk4-1763883398901-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff2a8f288012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_428c586e4cd14549b78aca3661693664',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:36","timestampInMs":1763883396801,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:36:36 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '8',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '20',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_52aa463522394d62a7993c6f77d12140',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=rO.6hzRalRW.pFzwFoD7D7fwUBOvlnXVpXALcwbu5lY-1763883396-1.0.1.1-wWXHa5Yf35nwUxXNB5eIqt3v7g9_nayWUAaSbjFoh8Il0d467UDV76VS4mH4lSW3u_laiTBX2KVeIWuZAzcHYKhcvCqeHzgU6AfRxkZDGLM; path=/; expires=Sun, 23-Nov-25 08:06:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=ArgtV6vlgscAqgss1NoXiWCi9lNrQ66fqOn2toJAJ1Q-1763883396797-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2eff1d783d8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_52aa463522394d62a7993c6f77d12140',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:36:36","timestampInMs":1763883396660,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"sq44x-1763883396604-da1a02cba03b","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":13634,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT9BHMYMYGPDTTE2PFYBV2","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:35:42","timestampInMs":1763883342500,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 321,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:42","timestampInMs":1763883342459,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:35:42 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '26',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_1bf8587b0c95458c8d956eba6835c351',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=A.xh54euEs_2err5P_HdClhYM1GoJev7PR1iLpEuUG8-1763883342-1.0.1.1-Rssm9oUUL0N213MsgXCZj8BPHY7R0SCDrmP4TDdeJYkRX3pGxvnqZPsefBp1UFYbVQCNtfQhwMNRBrB5BBjPd9G1evAr2N4tKA.LAsuNQ0I; path=/; expires=Sun, 23-Nov-25 08:05:42 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=3X_u8s_EOfNuKRcPJX0fWqzPFLXKDHAgY1Ceh2eur5E-1763883342455-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efdca091c8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_1bf8587b0c95458c8d956eba6835c351',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:42","timestampInMs":1763883342399,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 6/7: gpt-5-mini","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:40","timestampInMs":1763883340287,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:40","timestampInMs":1763883340287,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:35:40 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '8',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '29',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_9af50103df5c4112b7bc52ead9e4e906',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=A_Dr_WFGBdXWTV5jFLrXq28NTdm2rp3KXy15IMqs8Ps-1763883340-1.0.1.1-jIWvHcDr5W9AG01qItQtWvuBfo9bBypvv5HJEoaZ4OUCszOAYNjt1NMOd338sWPemiKgImpE5dNkPeRfeGiduFIL3AcpzKz3J5IB3NxQNBE; path=/; expires=Sun, 23-Nov-25 08:05:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=no..Yd2ZcSCIy4tWAFBztvt99_KzKGlHvqy89jhJJkg-1763883340283-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efdbc598b8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_9af50103df5c4112b7bc52ead9e4e906',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:36","timestampInMs":1763883336211,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:35:36 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '15',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '68',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_c7acdf22b03d4306b89a6aed2321749c',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=GzQX3nG8tTSr78kIydsAbNiaxXyONq06yR6dg1pZbWI-1763883336-1.0.1.1-uBG3r1Ai7ZTYZtDDEccG3ntQxf0cBQk4V3FeMIsV4BXvsSiBZXXRqemZJE34KEj1_4vmcIU5LK0Tq8KmvwbyJ1ANmmmIqUx_mORxFRooXrc; path=/; expires=Sun, 23-Nov-25 08:05:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=1HqVd2coIra9rKOFKJwQXPgUCMZn9NcqyYcx3VvxO2o-1763883336207-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efda2bc368012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_c7acdf22b03d4306b89a6aed2321749c',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:34","timestampInMs":1763883334112,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:35:34 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '30',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_09dd53c58bcd4f569f35025d920cb395',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=OXDJopvaqNIkbvguECYjrYLbAehAhFxUbfURP8tGEXY-1763883334-1.0.1.1-Gof.SnstVgIzBvh96PUV5ZfR8GxQY6lrnlcwe6w3NoLlXeIVeatGXrB3QnAepXMmOIokHBI.S_KXLPp9kirPhUYowjaugow3El0Jtg0.vhY; path=/; expires=Sun, 23-Nov-25 08:05:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=fTAFyj7pBbI1i13PTq7Q0kXTrwQZU0dQDa5oxvUu_xI-1763883334107-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efd95ad4b8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_09dd53c58bcd4f569f35025d920cb395',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:33","timestampInMs":1763883333018,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:35:33 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '13',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '230',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '150ms',\n    'x-request-id': 'req_45ed63c7e3654287aea5868b3331e6b8',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=8vpxcQQgUFmGRwUbLBNEqqRp.wIW4UhmMoT7Y4DH14g-1763883333-1.0.1.1-.2kPvcRLPpWRiLP.wlyVnj50.5enAa9tUBpp0hSBR5WD0m_fOIeqvYofqRzGmbH829XH53LL54YQWZeUrTcqKPTHFf0H2eD8DqiYpMbrfIE; path=/; expires=Sun, 23-Nov-25 08:05:33 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=.n6JiEUpFlWphkMiNm5NksNZi_c0JqgIAVwZYzjJomA-1763883333014-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efd8a0e758012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_45ed63c7e3654287aea5868b3331e6b8',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:32","timestampInMs":1763883332045,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:32","timestampInMs":1763883332045,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:35:32 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '32',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_5d2902ae08d04dc9836da0326deadbef',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=lQfzqu2bFqbVKk46yWexk9JwG2LM6mvtrpWC_.rIvuM-1763883332-1.0.1.1-SAbmvimiTzD0suXHBMznFQftjk_ftwhrDl3QxPs1HjEgiGGhLjQO55tP9WEaHg3q4Wt_K6HXnSCIykqhz46LvPCJ_bt7IR06ZwK6jmYeGGQ; path=/; expires=Sun, 23-Nov-25 08:05:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=aJl_12xJYaFS_lG4Yt1JPcW9nOBwHeRacFLStwMeFHk-1763883332041-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efd88bdaa8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_5d2902ae08d04dc9836da0326deadbef',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:35:31","timestampInMs":1763883331930,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"2rd7p-1763883331871-c4406c6f6eb1","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":10891,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT7CATRW5NQF5CXRP2WP6Y","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:33:55","timestampInMs":1763883235765,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 321,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:54","timestampInMs":1763883234676,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:54 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '13',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '32',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_92e922d041194a1ba8045aa9ec21bfd4',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=5Vmv73qV2XTRa78L23.x3myqE7YbfBFlPMJoS2B.pxI-1763883234-1.0.1.1-.YXBxhP.qiStUKjGATV2X1nBnmgEXrVmbsDeUZk0HBzeDHn0J2iXeASOvsyR1iuVWoMyta6Hdtfrxa3n7pHlkrkAwzzkEjKJpewgkklyHmw; path=/; expires=Sun, 23-Nov-25 08:03:54 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=DIka7WOqcAX5blWVRq7AqIzJu94JGDZjYL5ptbxdXMM-1763883234672-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efb283f4d8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_92e922d041194a1ba8045aa9ec21bfd4',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:52","timestampInMs":1763883232590,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:52 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '44',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498879',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_e82e99275ab341958233a830c915dbb3',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=EGwQM03joYOgdT3i3gnobPyHcGb6gI2PBaRuXmkLlbc-1763883232-1.0.1.1-utxB9yXAHOso4KnN5_FlWpQQJNObtO1tPaVTYo_K7rllRe7Vcpykap0fFdw.MZCQkJFnPOkXdCs2eJWX2AXVI0Rk2YY6RuBoDEZpK0pNpdY; path=/; expires=Sun, 23-Nov-25 08:03:52 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=UK16SqojYQXkndtdtfWqeqWKZZBVg5P0cZv.WDV9CYQ-1763883232586-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efb1b08c18012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_e82e99275ab341958233a830c915dbb3',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:51","timestampInMs":1763883231484,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:51 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '14',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '32',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '500000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '498880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '134ms',\n    'x-request-id': 'req_1627e1375ef94d928ea32ebdf77fe54c',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=KeFLWykvqtHESdLTOFasdAmOnZy8YAYbyMXXmsftixg-1763883231-1.0.1.1-fH34jAEhbK5zAmKdkTp_yQ3h8RpN4GxkWSKW7XsYgbiA7Oa4fWQLTFlxbpXkrLtxgef9NqfDBe7Vl.E0dNphYhMQaEq2F2owVUINA2EbdJw; path=/; expires=Sun, 23-Nov-25 08:03:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=yHEdBc14h0cDE6mHsxr0maGiuS91XUwfxUul6r2hH4g-1763883231480-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efb146d918012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_1627e1375ef94d928ea32ebdf77fe54c',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:51","timestampInMs":1763883231418,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 5/7: gpt-5","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:49","timestampInMs":1763883229262,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:49","timestampInMs":1763883229259,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:49 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '234',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.504s',\n    'x-request-id': 'req_216c898c48524764ab3676afa1f24a50',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=j.wBD6j9uMTXDgFMMZ5pUWA2KUpSEXLD45X82qb9E8Q-1763883229-1.0.1.1-8smdX_FrDBy8WUXV3_Z8e9d_8nHvEQcNT.Kg0r4E74eU8.1g5o4sw4vj..EjeveYIQf5oMen9TFBJZ_pRmfpwnbubqY8_hlrYs6mf_2ylvA; path=/; expires=Sun, 23-Nov-25 08:03:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=Y7QM3Yk2VsZgdB8eimx81j409ArKmje1YvMgWsuwIF4-1763883229255-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efb019c7e8012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_216c898c48524764ab3676afa1f24a50',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:44","timestampInMs":1763883224409,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:44 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '8',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '21',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.504s',\n    'x-request-id': 'req_555f2d8d55c7465b9ba509c69a83b6f2',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=nbTlIMqK_9y9SjeYf5BicJdFH_JP2iki2Ufaai1Qm5o-1763883224-1.0.1.1-e3PsD7nhEOY5J_mg83EnrgZMMnnZaYK9MmXUF4do7hA6W0s0E4n9Brwp.8uGUhjhZKOwgdV4jrgXuoB.ik61UTdMYYawzRIUDxH_45sz4HU; path=/; expires=Sun, 23-Nov-25 08:03:44 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=F5_DwF0vaPItwf1NdjDOujHWZd.K9uuykP3EBKbp1NY-1763883224405-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efae80fb78012-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_555f2d8d55c7465b9ba509c69a83b6f2',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:44","timestampInMs":1763883224292,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"sxjjz-1763883224230-5c331b3840f9","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":11602,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT4374S8XTM3PRYDYEYJZG","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:33:18","timestampInMs":1763883198827,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 319,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:17","timestampInMs":1763883197495,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:17 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '280',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '27417',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '5.165s',\n    'x-request-id': 'req_2b6b1fcac6ec49faa19686a484af3ea3',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=bNGqK9gDGRqEgVh1nCM5ObyjCGQRNWS.3OeLjKTRti8-1763883197-1.0.1.1-jQ5HSovqVNvgAEj.gtLlrhqUol4w0PDFohrCRw1V2VF20aLcJW8ra5d6HhlD2UaThKMXwYvOGmADF425UBjRuhFmntcYf955E3SXqko.VVo; path=/; expires=Sun, 23-Nov-25 08:03:17 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=K9Fn0Exilqr2mIP3xLFNMUa2IkbKIR6Uv9ZVcYbVJy4-1763883197491-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efa3aaeafbacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_2b6b1fcac6ec49faa19686a484af3ea3',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:15","timestampInMs":1763883195579,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:15 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '7',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '19',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.504s',\n    'x-request-id': 'req_148bc05c978b4ec889dbfef9b47f8b37',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=b.tdT2V59gWJn5Rq.22ZX.X4knZ7aOxloD.vPv68Lxc-1763883195-1.0.1.1-VmcVImpM_VwX7FjSM13In_hOAFnOzsIOALcnKZKN_La2CWCwsoZH7H.Esr8jC9pz2wEevTPmhqmsYN3WpvrP2kNGMJRdPaznais_GnY5ikQ; path=/; expires=Sun, 23-Nov-25 08:03:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=bXJ5d6QI17fArqvHWmkFw11uyxbO1FhLs3VjpR6_G3Q-1763883195575-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efa341cd6bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_148bc05c978b4ec889dbfef9b47f8b37',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:15","timestampInMs":1763883195416,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:15","timestampInMs":1763883195416,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:15 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '26',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.24s',\n    'x-request-id': 'req_131d31682f624984b2d1b37bb15f68c6',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=Ri9cEmuUVbiApTWMkqpT9RCQSHneqkBWk8SKB4A1hXA-1763883195-1.0.1.1-BPZVX5roSFH6L2yh_p3XktjQZ3bKQMlwQYndTRKd0k.d856ZDGKuvaxTaDAXw9cPWRaG3CHEw7EAiOQj444gGqawqK.wOwkmhktz142GkNA; path=/; expires=Sun, 23-Nov-25 08:03:15 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=zuMxZbumSHl6QNWCgoXSabpLnteEm77I3tzB24rc_K8-1763883195412-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efa330b49bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_131d31682f624984b2d1b37bb15f68c6',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:11","timestampInMs":1763883191362,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:11 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '24',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28205',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '3.588s',\n    'x-request-id': 'req_885444b1d8f540c4bd4304234cfd3fb6',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=W0DxtZem_AyKQX5z_r74jfJCOg0IaUd5vrFCCQo89hA-1763883191-1.0.1.1-RFpnu6JG_pQ.zEcnCxXJkfXAxc6wHUQCyDgEJpRF5Qx4zTFh.3.JSvHPRa0iUx1rlek7utwRFv6nFkc.SmQyDybLqcE9.iPJBXMQIhtm15I; path=/; expires=Sun, 23-Nov-25 08:03:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=CGajXCt0pgBaDKIZ1FaAih8oSp.Uw1IJ2Bv3ONc4yCk-1763883191358-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efa199c14bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_885444b1d8f540c4bd4304234cfd3fb6',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:09","timestampInMs":1763883189288,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:09 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '24',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28289',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '3.421s',\n    'x-request-id': 'req_99d5e238ac8044c79a0686bc1dadc7f0',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=TjULDnYtM.puhMcwxEuN5ReLn4IDh2gVam_xAAfte8c-1763883189-1.0.1.1-MNAyX.HzlSuy9IA4ogzYGv5oTaYnVikVQcGnDSu1YYZ0gUsdGxw0U.BJVxt.Vl513PfZpz2B_d6CvCUkuQCbS9R5HrhKwCt.lEEP1UvoSrM; path=/; expires=Sun, 23-Nov-25 08:03:09 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=mr3OnsM_vvrPTFLP8pQ1pEkiDLsi5IS7XqQh3fIhf9U-1763883189283-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efa0cb80dbacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_99d5e238ac8044c79a0686bc1dadc7f0',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:08","timestampInMs":1763883188233,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:08 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '265',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.24s',\n    'x-request-id': 'req_93ad88e9833b49ec8066b19cf9e0f50b',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=GpWFPyj74Su3TmQdTb37QFtNWRifCt3dWnt7yr_W4qQ-1763883188-1.0.1.1-xxNekTLmKtpM43UU7hNXrgL3IhlD3JC83_lExJOZEuf6gMLFCdamBp2GPSrmssuICWN8FTAz3HsuaF3RhqU45kKLUs1XncX8VEHI0mtcHX8; path=/; expires=Sun, 23-Nov-25 08:03:08 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=Ci_yxJJOUqhtSbfr7hU3nDDkPJtoeA93M8KvOQv6Gtk-1763883188229-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2efa00dcbfbacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_93ad88e9833b49ec8066b19cf9e0f50b',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:07","timestampInMs":1763883187333,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 4/7: gpt-5.1-instant","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:07","timestampInMs":1763883187314,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"mtmf6-1763883187257-8b13bb66a641","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":11620,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2Z3J5GCS991PWEBJMJN4","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:33:01","timestampInMs":1763883181836,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 321,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:01","timestampInMs":1763883181289,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:33:01","timestampInMs":1763883181288,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:33:01 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '14',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '312',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28748',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.504s',\n    'x-request-id': 'req_7de2d67a99df4f5aa303c485822ffcc8',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=5cs4wgqOCGIRRKhsI6KS9btkREs3KnBMiaMk6BnSeSw-1763883181-1.0.1.1-kby6Jum_0svH8cFA3VL8nzET0ynuhgQFwumjQEeeM0iwAIGIv9Zpdk5EUbQBT1on4WHDOoHPdtv0Ep9um6XQUHIuVFiJeWH4LHt5IC.JrcY; path=/; expires=Sun, 23-Nov-25 08:03:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=_T7d54kgWYxcp7TOwZKHf2Zbb.TFQW_fj7jfJqF4cEI-1763883181285-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef9cd9841bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_7de2d67a99df4f5aa303c485822ffcc8',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:55","timestampInMs":1763883175133,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:55 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '287',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '27722',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '4.555s',\n    'x-request-id': 'req_20066486f9be40e99d3664806fc69222',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=ngff8WdkR_wmKd337MRVC_Ct4W19rcSn3YX7MQjtWhk-1763883175-1.0.1.1-UnyVI4D6FUcvcSUND_E5tV3seehl0u_45PLOW2wGP1WEtqxkC4IuFadc8Ve_eWCkOZfhwHX7GK2qotzxsWVRfgPct0lTlPU4SYO.2p5djV0; path=/; expires=Sun, 23-Nov-25 08:02:55 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=7I6HOHDXQXcwwm4QALMmYM3hB0aiuxnE5doLaiOmHk8-1763883175129-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef9ae2e19bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_20066486f9be40e99d3664806fc69222',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:52","timestampInMs":1763883172102,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:52 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '10',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '24',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '27443',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '5.113s',\n    'x-request-id': 'req_10e8ffca965049abb2b64521f1db1f21',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=.6xUhsomUbZLBdjtDFMp.6SPqC3fUwGGsqeLV.E.7n4-1763883172-1.0.1.1-iCPa2t8Dju2Hd3lQIbdHj96gZ4UIqAMWBeEAhlJvgKT1Hj_i234OvxvrpFmLYnB.1I4w2d7uX_ztSILNCwsMwZJSK926gvVNsbGj0WDp8Pg; path=/; expires=Sun, 23-Nov-25 08:02:52 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=DRxNl8gFArSmqpd3drAWkVW2F.u8oizUGweirSFPsqc-1763883172097-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef9a10a70bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_10e8ffca965049abb2b64521f1db1f21',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:51","timestampInMs":1763883171003,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:52:33)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:51 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '234',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28153',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '3.692s',\n    'x-request-id': 'req_c0a69f990a6c4c6ebded3d7945928ce4',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=sxj5Lr.Uy02D98I9Ek3KBNrYVcfhs2gqPCBGUegm22A-1763883170-1.0.1.1-qu9QxWa7ZeHTIQgaYMMvbz7hw7rqHPlhOh2.7xwtWWix3nlV2ZI5ExD0NSQTYYoSz1JuqRgZMBEzdBB.zgd3nKzZzxrRHX.DbCVz4pFyR0c; path=/; expires=Sun, 23-Nov-25 08:02:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=GPWEKt4eJsgsivEUsK65TUQfvp8nlC7Zxg7W48_gvL4-1763883170999-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef9950e5dbacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_c0a69f990a6c4c6ebded3d7945928ce4',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:49","timestampInMs":1763883169951,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error generating summary with AI: Error: AI API failed: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at generateAICompletion (/var/task/src/services/ai-provider.js:158:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810)","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:49","timestampInMs":1763883169951,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API failed after 4 attempts: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:49 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '8',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '22',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.24s',\n    'x-request-id': 'req_cfaee1c473fe445fbff4f80fb4ea7541',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=_xFRgdCDVKjj4q8ctp5G3EoURPTJ9hFvhrGo5r6.jLU-1763883169-1.0.1.1-rNIhRFrvbpm2xLB99MD0Lt_LPkbzANI3XqBvGdBbw.8Ty_KhWagepmfsSOLkMOFZI8VME0tL7nSrmkoYt3Z5nBM.JSWSZnGDK3zOGiHG02I; path=/; expires=Sun, 23-Nov-25 08:02:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=ppd_tyq_fIE2e4rNzeTMwhfeF5Rc9vWICyceH.qUmF8-1763883169946-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef993ac43bacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_cfaee1c473fe445fbff4f80fb4ea7541',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:49","timestampInMs":1763883169848,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"sxjjz-1763883169664-091d78bc1d65","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":12090,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT2E1R17VS44S4HMBTZQB8","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:32:32","timestampInMs":1763883152512,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 322,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:29","timestampInMs":1763883149093,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 3/4), retrying in 4000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:29 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '24',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '423',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.24s',\n    'x-request-id': 'req_505dfbe44b6b4d869b1a2ceee17f77db',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=febQG.RP8qFxxRFps9mQhVCJUTMde4y7kkbt.WXWCkg-1763883149-1.0.1.1-pGEhzduCNEoBSbVWfCpRiK77qy.szNYlssCgOWWhbvYpNfcyupESHDEeV0qsDqNgwZWmzVtQn8ErbVDWdoM5M8gBr4B01ucb8gJ1LOpJXzo; path=/; expires=Sun, 23-Nov-25 08:02:29 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=pNCmv4G_W9LR3HOGXrC763otGLRGMOAkI1V7P8Tx6SI-1763883149088-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef90a9f1ebacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_505dfbe44b6b4d869b1a2ceee17f77db',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:25","timestampInMs":1763883145937,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 2/4), retrying in 2000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:25 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '9',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '140',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28566',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.867s',\n    'x-request-id': 'req_71132c52946445f1bb1343ed4139206f',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=mfQFJn14d08Avv0BWhCr4LU5PVGxlZqQBlH8vS4L9SE-1763883145-1.0.1.1-k4yKETgp1kHufD8Va8nhO3GWXOXFeaI8yw9gkLQKIqLgjpElSthbsdj32Rfd7ISmS4xUycSGaUsuubli0.M1js0nnTGYmGKFvfWXV5LHIj4; path=/; expires=Sun, 23-Nov-25 08:02:25 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=o5WLGXZSDWHRLfYLVtcQ9R6ULzdxHFHwLq8ugj50sT4-1763883145932-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef8fa3ceabacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_71132c52946445f1bb1343ed4139206f',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:24","timestampInMs":1763883144313,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"warning","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI API error (attempt 1/4), retrying in 1000ms: BadRequestError: 400 Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\n    at APIError.generate (/var/task/node_modules/openai/core/error.js:45:20)\n    at OpenAI.makeStatusError (/var/task/node_modules/openai/client.js:165:32)\n    at OpenAI.makeRequest (/var/task/node_modules/openai/client.js:333:30)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async callOpenAI (/var/task/src/services/ai-provider.js:84:24)\n    at async generateAICompletion (/var/task/src/services/ai-provider.js:132:19)\n    at async callAI (/var/task/src/services/claude.js:90:24)\n    at async generateSummary (/var/task/src/services/claude.js:116:12)\n    at async testSingleModel (/var/task/src/services/model-tester.js:37:30)\n    at async testModels (/var/task/src/services/model-tester.js:87:13)\n    at async handleTestModelsCommand (/var/task/src/services/telegram.js:175:9)\n    at async Object.handler (/var/task/api/webhook.js:67:13)\n    at async r (/opt/rust/nodejs.js:2:15568)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:2:11593)\n    at async Server.<anonymous> (/opt/rust/nodejs.js:16:7810) {\n  status: 400,\n  headers: Headers {\n    date: 'Sun, 23 Nov 2025 07:32:24 GMT',\n    'content-type': 'application/json',\n    'content-length': '245',\n    connection: 'keep-alive',\n    'access-control-expose-headers': 'X-Request-ID',\n    'openai-organization': 'user-cv2hc28nzkspzqnv7rjpi4ml',\n    'openai-processing-ms': '13',\n    'openai-project': 'proj_x8MgdBAnWeIzLA7qWWUUkrUw',\n    'openai-version': '2020-10-01',\n    'x-envoy-upstream-service-time': '327',\n    'x-ratelimit-limit-requests': '500',\n    'x-ratelimit-limit-tokens': '30000',\n    'x-ratelimit-remaining-requests': '499',\n    'x-ratelimit-remaining-tokens': '28880',\n    'x-ratelimit-reset-requests': '120ms',\n    'x-ratelimit-reset-tokens': '2.24s',\n    'x-request-id': 'req_b305a063ccec4875a632ed0e56d2c24c',\n    'x-openai-proxy-wasm': 'v0.1',\n    'cf-cache-status': 'DYNAMIC',\n    'set-cookie': '__cf_bm=aI0.SG6TAPt1SM3wkOm4fS2DNEXX..u0p4oXMXyUTaA-1763883144-1.0.1.1-Ua0b_8W3A8_OhcKXhS6d8L6qVqo8lIjss9El5zz1nkSq29Ps6PpXdk4vgKBVLHQdASW2g1VBDZNUw3ZgsLEeCCSoM669wPdXeU_4eN1zcpY; path=/; expires=Sun, 23-Nov-25 08:02:24 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None, _cfuvid=nGynBBe9UbHjzV2d7_da5s9m1H23xllb99xXlVFUBVA-1763883144307-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None',\n    'strict-transport-security': 'max-age=31536000; includeSubDomains; preload',\n    'x-content-type-options': 'nosniff',\n    server: 'cloudflare',\n    'cf-ray': '9a2ef8ec8e3cbacc-IAD',\n    'alt-svc': 'h3=\":443\"; ma=86400'\n  },\n  requestID: 'req_b305a063ccec4875a632ed0e56d2c24c',\n  error: {\n    message: \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\",\n    type: 'invalid_request_error',\n    param: 'max_tokens',\n    code: 'unsupported_parameter'\n  },\n  code: 'unsupported_parameter',\n  param: 'max_tokens',\n  type: 'invalid_request_error'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:23","timestampInMs":1763883143106,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 3/7: gpt-5.1","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:20","timestampInMs":1763883140941,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4',\n  inputTokens: 1719,\n  outputTokens: 435,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:32:20","timestampInMs":1763883140923,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"6vx9n-1763883140736-17cea8d2c7b2","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":11694,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT1HSVKM0Z4JNCZY5RQG8B","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:31:44","timestampInMs":1763883104653,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mvclh-1763883094142-d958efb6783f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 322,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT04AT6EN8FQKD1EJDGMP3","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:31:43","timestampInMs":1763883103169,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mvclh-1763883094142-d958efb6783f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4',\n  inputTokens: 1437,\n  outputTokens: 308,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT04AT6EN8FQKD1EJDGMP3","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:31:36","timestampInMs":1763883096693,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mvclh-1763883094142-d958efb6783f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 2/7: claude-sonnet-4","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT04AT6EN8FQKD1EJDGMP3","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:31:34","timestampInMs":1763883094388,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"mvclh-1763883094142-d958efb6783f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1719,\n  outputTokens: 431,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT04AT6EN8FQKD1EJDGMP3","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:31:34","timestampInMs":1763883094362,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"mvclh-1763883094142-d958efb6783f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":10393,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQT04AT6EN8FQKD1EJDGMP3","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:31:03","timestampInMs":1763883063946,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"5q7p7-1763883048165-09a519918077","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 319,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSYQ91027MRTE33A3A5BS7","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:31:03","timestampInMs":1763883063077,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"5q7p7-1763883048165-09a519918077","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 319,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSYQ91027MRTE33A3A5BS7","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:30:55","timestampInMs":1763883055864,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"5q7p7-1763883048165-09a519918077","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Testing model 1/7: claude-sonnet-4.5","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSYQ91027MRTE33A3A5BS7","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:30:48","timestampInMs":1763883048225,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"5q7p7-1763883048165-09a519918077","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":15837,"region":"iad1","maxMemoryUsed":367,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSYQ91027MRTE33A3A5BS7","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:30:35","timestampInMs":1763883035563,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"w2k2q-1763883035368-95fc1cafb517","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"[dotenv@17.2.3] injecting env (0) from .env -- tip:  add observability to secrets: https://dotenvx.com/ops","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSYAWVVKV8V2FWA3VSKJ56","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:30:35","timestampInMs":1763883035547,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"w2k2q-1763883035368-95fc1cafb517","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":121,"region":"iad1","maxMemoryUsed":342,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSYAWVVKV8V2FWA3VSKJ56","instanceId":"aNPHoXGyW6rH","concurrency":1},{"TimeUTC":"2025-11-23 07:30:16","timestampInMs":1763883016240,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"4mmxz-1763883004264-12d18d1a89d0","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"AI Completion Success: {\n  model: 'Claude Sonnet 4.5',\n  inputTokens: 1437,\n  outputTokens: 321,\n  stopReason: 'end_turn',\n  attempt: 1\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSXCP8GF0A52KYAR54QFAY","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:30:04","timestampInMs":1763883004653,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"4mmxz-1763883004264-12d18d1a89d0","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"[dotenv@17.2.3] injecting env (0) from .env -- tip:  prevent committing .env to code: https://dotenvx.com/precommit","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSXCP8GF0A52KYAR54QFAY","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:30:04","timestampInMs":1763883004615,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"4mmxz-1763883004264-12d18d1a89d0","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AQiz1Hz2XzzgZE2CYPx29dDETiR2","durationMs":12472,"region":"iad1","maxMemoryUsed":433,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSXCP8GF0A52KYAR54QFAY","instanceId":"sdOzLN3NXXQ8","concurrency":1},{"TimeUTC":"2025-11-23 07:24:02","timestampInMs":1763882642591,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"544n2-1763882642534-37a67d8c448e","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AzbN9RBcSUYRDBC4M7ekQPYQU6YZ","durationMs":132,"region":"iad1","maxMemoryUsed":363,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSJB4ZSEKVBGWC5WANJXV0","instanceId":"Z7PlLhWdFyug","concurrency":1},{"TimeUTC":"2025-11-23 07:23:51","timestampInMs":1763882631314,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"78l69-1763882631175-7d1e55dbc2f8","requestUserAgent":"-","level":"error","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AzbN9RBcSUYRDBC4M7ekQPYQU6YZ","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"Error sending summary to user 762715667: RequestError: Error: Client network socket disconnected before secure TLS connection was established\n    at new RequestError (/var/task/node_modules/request-promise-core/lib/errors.js:14:15)\n    at plumbing.callback (/var/task/node_modules/request-promise-core/lib/plumbing.js:87:29)\n    at Request.RP$callback [as _callback] (/var/task/node_modules/request-promise-core/lib/plumbing.js:46:31)\n    at self.callback (/var/task/node_modules/@cypress/request/request.js:183:22)\n    at Request.emit (node:events:519:28)\n    at Request.onRequestError (/var/task/node_modules/@cypress/request/request.js:869:8)\n    at ClientRequest.emit (node:events:531:35)\n    at emitErrorEvent (node:_http_client:105:11)\n    at TLSSocket.socketErrorListener (node:_http_client:518:5)\n    at TLSSocket.emit (node:events:519:28)\n    at emitErrorNT (node:internal/streams/destroy:170:8)\n    at emitErrorCloseNT (node:internal/streams/destroy:129:3)\n    at process.processTicksAndRejections (node:internal/process/task_queues:90:21) {\n  code: 'EFATAL'\n}","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSJ025KB4MFR6QD20VB9J7","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:23:51","timestampInMs":1763882631237,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"78l69-1763882631175-7d1e55dbc2f8","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AzbN9RBcSUYRDBC4M7ekQPYQU6YZ","durationMs":75,"region":"iad1","maxMemoryUsed":352,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSJ025KB4MFR6QD20VB9J7","instanceId":"Z7PlLhWdFyug","concurrency":1},{"TimeUTC":"2025-11-23 07:21:27","timestampInMs":1763882487501,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":-1,"requestId":"zklzm-1763882484823-1d0a2125ec9f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AzbN9RBcSUYRDBC4M7ekQPYQU6YZ","durationMs":-1,"region":"iad1","maxMemoryUsed":-1,"memorySize":-1,"message":"[dotenv@17.2.3] injecting env (0) from .env -- tip:   load multiple .env files with { path: ['.env.local', '.env'] }","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSDH913WJJVVF6559MGF45","instanceId":"","concurrency":-1},{"TimeUTC":"2025-11-23 07:21:25","timestampInMs":1763882485025,"requestPath":"famcalbot.vercel.app/api/webhook","requestMethod":"POST","requestQueryString":"","responseStatusCode":200,"requestId":"zklzm-1763882484823-1d0a2125ec9f","requestUserAgent":"-","level":"info","environment":"production","branch":"main","vercelCache":"","type":"lambda","function":"/api/webhook.ts","host":"famcalbot.vercel.app","deploymentDomain":"famcalbot.vercel.app","deploymentId":"dpl_AzbN9RBcSUYRDBC4M7ekQPYQU6YZ","durationMs":2344,"region":"iad1","maxMemoryUsed":351,"memorySize":2048,"message":"","projectId":"prj_48JScp5UkKp2qpAPMFwpQAYcJcW9","traceId":"","sessionId":"","invocationId":"01KAQSDH913WJJVVF6559MGF45","instanceId":"Z7PlLhWdFyug","concurrency":1}]